<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta name="description" content="Object-Oriented Software" />
	<meta name="keywords" content="Design, Patterns, Elements, Reusable, Object-Oriented, Software" />
	<title>Thinking About Performance</title>
	<link rel="stylesheet" href="css/reset.css" />
	<link rel="stylesheet" href="css/style.css" />
</head>
<body>
	<header>
		<div class="container">
			<a href="#" class="logo">Martin <span>Fowler</span></a>
			<a href="index.html" class="home-link">&nbsp;</a>
		</div>
	</header>
	<main>
		<section class="content-section">
			<div class="container">
				<h1>Thinking About Performance</h1>
				<div class="wrap-image">
					<img src="img/image_03.jpg" alt="">
				</div>
				<p>Many architectural decisions are about performance. For most performance issues I prefer to get a system	up and running, instrument it, and then use a disciplined optimization process based on measurement.	However, some architectural decisions affect performance in a way that's difficult to fix with later	optimization. And even when it is easy to fix, people involved in the project worry about these decisions	early.</p>
				<p>It's always difficult to talk about performance in a book such as this. The reason that it's so difficult is that	any advice about performance should not be treated as fact until it's measured on your configuration. Too	often I've seen designs used or rejected because of performance considerations, which turn out to be bogus	once somebody actually does some measurements on the real setup used for the application.</p>
				<p>I give a few guidelines in this book, including minimizing remote calls, which has been good performance	advice for quite a while. Even so, you should verify every tip by measuring on your application. Similarly	there are several occasions where code examples in this book sacrifice performance for understandability.	Again it's up to you to apply the optimizations for your environment. Whenever you do a performance	optimization, however, you must measure both before and after, otherwise, you may just be making your	code harder to read.</p>
				<p>There's an important corollary to this: A significant change in configuration may invalidate any facts about	performance. Thus, if you upgrade to a new version of your virtual machine, hardware, database, or almost	anything else, you must redo your performance optimizations and make sure they're still helping. In many	cases a new configuration can change things. Indeed, you may find that an optimization you did in the past	to improve performance actually hurts performance in the new environment.</p>
				<p>Another problem with talking about performance is the fact that many terms are used in an inconsistent	way. The most noted victim of this is "scalability," which is regularly used to mean half a dozen different	things. Here are the terms I use.</p>
				<p>
					<b>Response time</b>
					is the amount of time it takes for the system to process a request from the outside. This	may be a UI action, such as pressing a button, or a server API call.</p>
				<p>
					<b>Responsiveness</b>
					is about how quickly the system acknowledges a request as opposed to processing it. This	is important in many systems because users may become frustrated if a system has low responsiveness,	even if its response time is good. If your system waits during the whole request, then your responsiveness	and response time are the same. However, if you indicate that you've received the request before you	complete, then your responsiveness is better. Providing a progress bar during a file copy improves the	responsiveness of your user interface, even though it doesn't improve response time.</p>
				<p>
					<b>Latency</b>
					is the minimum time required to get any form of response, even if the work to be done is	nonexistent. It's usually the big issue in remote systems. If I ask a program to do nothing, but to tell me	when it's done doing nothing, then I should get an almost instantaneous response if the program runs on my	laptop. However, if the program runs on a remote computer, I may get a few seconds just because of the	time taken for the request and response to make their way across the wire. As an application developer, I	can usually do nothing to improve latency. Latency is also the reason why you should minimize remote calls.</p>
				<p>
					<b>Throughput</b>
					is how much stuff you can do in a given amount of time. If you're timing the copying of a file,	throughput might be measured in bytes per second. For enterprise applications a typical measure is	transactions per second (tps), but the problem is that this depends on the complexity of your transaction.	For your particular system you should pick a common set of transactions.</p>
				<p>In this terminology 
					<b>performance</b>
					is either throughput or response time—whichever matters more to you. It	can sometimes be difficult to talk about performance when a technique improves throughput but decreases	response time, so it's best to use the more precise term. From a user's perspective responsiveness may be	more important than response time, so improving responsiveness at a cost of response time or throughput	will increase performance.</p>
				<p>
					<b>Load</b>
					is a statement of how much stress a system is under, which might be measured in how many users are	currently connected to it. The load is usually a context for some other measurement, such as a response	time. Thus, you may say that the response time for some request is 0.5 seconds with 10 users and 2	seconds with 20 users.</p>
				<p>
					<b>Load sensitivity</b>
					is an expression of how the response time varies with the load. Let's say that system A has	a response time of 0.5 seconds for 10 through 20 users and system B has a response time of 0.2 seconds for	10 users that rises to 2 seconds for 20 users. In this case system A has a lower load sensitivity than system	B. We might also use the term 
					<b>degradation</b>
					to say that system B degrades more than system A.
				</p>
				<div id="image_07" class="wrap-image">
					<img src="img/image_07.jpg" alt="">
				</div>
				<p>
					<b>Efficiency</b>
					is performance divided by resources. A system that gets 30 tps on two CPUs is more efficient	than a system that gets 40 tps on four identical CPUs.</p>
				<p>The 
					<b>capacity</b>
					of a system is an indication of maximum effective throughput or load. This might be an	absolute maximum or a point at which the performance dips below an acceptable threshold.</p>
				<p>
					<b>Scalability</b>
					is a measure of how adding resources (usually hardware) affects performance. A scalable system	is one that allows you to add hardware and get a commensurate performance improvement, such as	doubling how many servers you have to double your throughput. 
					<b>Vertical scalability,</b> or <b>scaling up,</b>
					means adding more power to a single server, such as more memory. 
					<b>Horizontal scalability,</b> or <b>scaling	out,</b>
					means adding more servers.</p>
				<p>The problem here is that design decisions don't affect all of these performance factors equally. Say we have	two software systems running on a server: Swordfish's capacity is 20 tps while Camel's capacity is 40 tps.	Which has better performance? Which is more scalable? We can't answer the scalability question from this	data, and we can only say that Camel is more efficient on a single server. If we add another server, we	notice that swordfish now handles 35 tps and camel handles 50 tps. Camel's capacity is still better, but	Swordfish looks like it may scale out better. If we continue adding servers we'll discover that Swordfish gets	15 tps per extra server and Camel gets 10. Given this data we can say that Swordfish has better horizontal	scalability, even though Camel is more efficient for less than five servers.</p>
				<p>When building enterprise systems, it often makes sense to build for hardware scalability rather than capacity	or even efficiency. Scalability gives you the option of better performance if you need it. Scalability can also	be easier to do. Often designers do complicated things that improve the capacity on a particular hardware	platform when it might actually be cheaper to buy more hardware. If Camel has a greater cost than	Swordfish, and that greater cost is equivalent to a couple of servers, then Swordfish ends up being cheaper	even if you only need 40 tps. It's fashionable to complain about having to rely on better hardware to make	our software run properly, and I join this choir whenever I have to upgrade my laptop just to handle the	latest version of Word. But newer hardware is often cheaper than making software run on less powerful	systems. Similarly, adding more servers is often cheaper than adding more programmers—providing that a	system is scalable.</p>
			</div>
		</section>
	</main>
	<footer>
		<div class="container">
			<p>&#169; Copyright 2016 &nbsp; <a href="#">#HardhatsTeam</a></p>
		</div>
	</footer>
</body>
</html>